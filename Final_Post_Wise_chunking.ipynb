{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Post_Wise_chunking.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPbEHxFR9Mo3Rs/Xf4ELiHM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prasanth59/Work/blob/master/Final_Post_Wise_chunking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "II08yvpLDEMC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e91f98f-171d-45d1-a539-647216e218b8"
      },
      "source": [
        "!pip install contractions\n",
        "!pip install langdetect"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/0a/04/d5e0bb9f2cef5d15616ebf68087a725c5dbdd71bd422bcfb35d709f98ce7/contractions-0.0.48-py2.py3-none-any.whl\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading https://files.pythonhosted.org/packages/d3/fe/021d7d76961b5ceb9f8d022c4138461d83beff36c3938dc424586085e559/textsearch-0.0.21-py2.py3-none-any.whl\n",
            "Collecting pyahocorasick\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7f/c2/eae730037ae1cbbfaa229d27030d1d5e34a1e41114b21447d1202ae9c220/pyahocorasick-1.4.2.tar.gz (321kB)\n",
            "\u001b[K     |████████████████████████████████| 327kB 4.3MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/c7/61370d9e3c349478e89a5554c1e5d9658e1e3116cc4f2528f568909ebdf1/anyascii-0.1.7-py3-none-any.whl (260kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 6.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85392 sha256=df3b9476a938824f7b4b808c36a8b345f7633a3c97a50b1c846d0fdc2b2fa64d\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/03/34/77e3ece0bba8b86bfac88a79f923b36d805cad63caeba38842\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.1.7 contractions-0.0.48 pyahocorasick-1.4.2 textsearch-0.0.21\n",
            "Collecting langdetect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/a3/8407c1e62d5980188b4acc45ef3d94b933d14a2ebc9ef3505f22cf772570/langdetect-1.0.8.tar.gz (981kB)\n",
            "\u001b[K     |████████████████████████████████| 983kB 6.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from langdetect) (1.15.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.8-cp37-none-any.whl size=993193 sha256=fcb71bea7a1a7a6ec0b6406ea4d6d746d2de5586b1d4f3a7bce9e872dbd66327\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/b3/aa/6d99de9f3841d7d3d40a60ea06e6d669e8e5012e6c8b947a57\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Gyhfo82DJlP",
        "outputId": "e5275388-a8bf-49cc-857d-17b6dd0a9837"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejG8cFQSDOOk"
      },
      "source": [
        "# !cp \"/content/english_blog_corpus_dict.pickle\" \"/content/drive/MyDrive/blog_post_wise_dict.pickle\" \n",
        "!cp '/content/drive/MyDrive/all_processed_final_min_sentence_length_dict.pickle' '/content/all_processed_final_min_sentence_length_dict.pickle'  \n",
        "# !cp \"/content/drive/MyDrive/blogs.zip\" \"/content/blogs.zip\" \n",
        "# !unzip '/content/blogs.zip'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFo_bu4JELJc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fd62528-4eaa-47dd-825f-3b0bc47f802b"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import os \n",
        "import pandas as pd\n",
        "import contractions\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "# from spacy.lang.en.stop_words import STOP_WORDS\n",
        "import pickle\n",
        "from langdetect import detect\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XSKQTiCw6P9"
      },
      "source": [
        "# Extract content from post tag in blog"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-UfefsJEeCK"
      },
      "source": [
        "def get_file_content(path,file_names,en):\n",
        "  \"\"\"Returns blog content in <p> tags in html file \"\"\"\n",
        "  file_text = {}\n",
        "  lang_exception = []\n",
        "  for file_name in file_names:\n",
        "    if file_name.strip().split(\".\")[0] in en:\n",
        "      posts_list = []\n",
        "      try:\n",
        "        with open(path+file_name, \"r\",encoding='utf-8-sig',errors='ignore') as file_obj:\n",
        "          contents = file_obj.read()\n",
        "          soup = BeautifulSoup(contents, 'lxml')\n",
        "          for tag in soup.find_all(\"post\"):\n",
        "            temp = tag.text.replace(\"\\\\\", \"\").replace(\"\\t\",'').replace(\"\\n\",'').replace('urlLink','').lower()\n",
        "            # print(temp)\n",
        "            if detect(temp)== 'en':\n",
        "              if len(temp.split()) > 10:\n",
        "                temp = ' '.join([(contractions.fix(word)) for word in temp.split()])\n",
        "                posts_list.append(temp)\n",
        "      except IOError:\n",
        "        print(\"Error: Input XML files not found \",file_name)\n",
        "        pass\n",
        "      except:\n",
        "        print('Language detection error',file_name.strip().split(\".\")[0])\n",
        "        lang_exception.append(file_name.strip().split(\".\")[0])\n",
        "        pass\n",
        "      # print(file_name.strip().split(\".\")[0])\n",
        "      file_text[file_name.strip().split(\".\")[0]] = posts_list\n",
        "  return file_text,lang_exception"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQlPoaW8EgWp"
      },
      "source": [
        "path = '/content/blogs/'\n",
        "files_list = os.listdir(path)\n",
        "total_content,lang_exception = get_file_content(path,files_list,file_chunk_content.keys())\n",
        "\n",
        "# pickle.dump(file_chunk_content,open('en_chunk_wise_dict.pickle','wb'),protocol= pickle.HIGHEST_PROTOCOL)\n",
        "len(lang_exception),len(total_content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_qxrKom7snp"
      },
      "source": [
        "# Method to detect the non english blogs in the corpus\n",
        "def remove_non_english_blog(file_dict):\n",
        "  non_en = []\n",
        "  for key in file_dict.keys():\n",
        "    try:\n",
        "      lang = detect(file_dict[key])\n",
        "      if lang!= 'en':\n",
        "        # print(lang,key)\n",
        "        non_en.append(key)\n",
        "    except :\n",
        "      print('Error in Key value',key)\n",
        "      non_en.extend(key)\n",
        "      continue\n",
        "  \n",
        "  for key in non_en:\n",
        "    try:\n",
        "      file_dict.pop(key)\n",
        "    except:\n",
        "       print('Error in Key value',key)\n",
        "       continue\n",
        "\n",
        "  return file_dict\n",
        "\n",
        "# file_chunk_content = remove_non_english_blog(file_chunk_content)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KInt7Fu4xgy-"
      },
      "source": [
        "# Perform preprocessing on the extracted chunks(posts) from blogs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oxU5Et6Fnqp"
      },
      "source": [
        "\n",
        "# Remove the punctuations from the sentences\n",
        "def remove_punctuation(content):\n",
        "  return ' '.join([token for token in content.split() if token.isalpha() == True])\n",
        "\n",
        "# Method to perform preprocessing task\n",
        "def preprocess_text(blog_text):\n",
        "  sentences = nltk.sent_tokenize(blog_text)\n",
        "  sentences = [remove_punctuation(sentence) for sentence in sentences]\n",
        "  sentences = ' '.join(sentences)\n",
        "  return sentences"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDmPFfGth4v2",
        "outputId": "4df0d36d-eb4c-4b66-807f-88c2d5f4c097"
      },
      "source": [
        "# Perform preprocessing on the blog content\n",
        "def get_preprocessed_content(file_chunk_content)\n",
        "testing_blogs = {}\n",
        "for index, key in enumerate(list(file_chunk_content.keys())):\n",
        "  testing_blogs[key] = list(map(preprocess_text,file_chunk_content[key]))\n",
        "  if index%1000 == 0:\n",
        "        print('Preprocessing done for ', index)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preprocessing done for  0\n",
            "Preprocessing done for  1000\n",
            "Preprocessing done for  2000\n",
            "Preprocessing done for  3000\n",
            "Preprocessing done for  4000\n",
            "Preprocessing done for  5000\n",
            "Preprocessing done for  6000\n",
            "Preprocessing done for  7000\n",
            "Preprocessing done for  8000\n",
            "Preprocessing done for  9000\n",
            "Preprocessing done for  10000\n",
            "Preprocessing done for  11000\n",
            "Preprocessing done for  12000\n",
            "Preprocessing done for  13000\n",
            "Preprocessing done for  14000\n",
            "Preprocessing done for  15000\n",
            "Preprocessing done for  16000\n",
            "Preprocessing done for  17000\n",
            "Preprocessing done for  18000\n",
            "Preprocessing done for  19000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ataD11enHoBs"
      },
      "source": [
        "# preprocess_pipeline = set_preprocess_pipeline()\n",
        "file_chunk_content = pickle.load(open('en_chunk_wise_dict.pickle','rb'))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrTl-PBhxjVw"
      },
      "source": [
        "pickle.dump(testing_blogs,open('all_processed_final_min_sentence_length_dict.pickle','wb'),protocol= pickle.HIGHEST_PROTOCOL)\n",
        "!cp \"/content/all_processed_final_min_sentence_length_dict.pickle\" \"/content/drive/MyDrive/all_processed_final_min_sentence_length_dict.pickle\" "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzUskxGeRcIS"
      },
      "source": [
        "testing_blogs = pickle.load(open('/content/all_processed_final_min_sentence_length_dict.pickle','rb'))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu89Iap-9quj"
      },
      "source": [
        "# Generate embeddings for each post (chunk) in the blog"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekiOW4SuVXVL"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow_hub as hub\n",
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/3\"\n",
        "# Import the Universal Sentence Encoder's TF Hub module\n",
        "embed = hub.load(module_url)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UepVgppgr1_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acbbaefd-9605-4490-a35a-aa523eb4f7c8"
      },
      "source": [
        "blog_embedding = {}\n",
        "for index, key in enumerate(list(testing_blogs.keys())):\n",
        "  # print(key)\n",
        "  blog_embedding [key] = [embed([post])['outputs'].numpy().reshape(1,-1) for post in testing_blogs[key]]\n",
        "  if index%1000 == 0:\n",
        "    print('Embeddings generated for ', index)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embeddings generated for  0\n",
            "Embeddings generated for  1000\n",
            "Embeddings generated for  2000\n",
            "Embeddings generated for  3000\n",
            "Embeddings generated for  4000\n",
            "Embeddings generated for  5000\n",
            "Embeddings generated for  6000\n",
            "Embeddings generated for  7000\n",
            "Embeddings generated for  8000\n",
            "Embeddings generated for  9000\n",
            "Embeddings generated for  10000\n",
            "Embeddings generated for  11000\n",
            "Embeddings generated for  12000\n",
            "Embeddings generated for  13000\n",
            "Embeddings generated for  14000\n",
            "Embeddings generated for  15000\n",
            "Embeddings generated for  16000\n",
            "Embeddings generated for  17000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzNazfVSo_QU"
      },
      "source": [
        "pickle.dump(blog_embedding,open('final_embeddings_dict.pickle','wb'),protocol= pickle.HIGHEST_PROTOCOL)\n",
        "!cp \"/content/final_embeddings_dict.pickle\" \"/content/drive/MyDrive/final_embeddings_dict.pickle\" \n",
        "# blog_embedding = pickle.load(open('embeddings_dict.pickle','rb'))\n",
        "# non_eng_list = pickle.load(open('non_english_list.pickle','rb'))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DE9RUH5pUCnO"
      },
      "source": [
        "for item in non_eng_list:\n",
        "  blog_embedding.pop(item)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "or3dxOeEhGdj"
      },
      "source": [
        "query = 'automobile'\n",
        "embed_query = embed([query])['outputs'][0].numpy().reshape(1,-1)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsGrLcAMiN4r"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "similarity = {}\n",
        "for key in blog_embedding.keys():\n",
        "  post_sim  = []\n",
        "  for item in blog_embedding[key]:\n",
        "    # print(item.shape,embed_query.shape)\n",
        "    # print(type(item),type(embed_query))\n",
        "    # print(item)\n",
        "    # break\n",
        "    post_sim.append(cosine_similarity(item.reshape(1,-1),embed_query))\n",
        "  try:  \n",
        "    similarity[key] = (np.max(post_sim),np.argmax(post_sim))\n",
        "  except ValueError:\n",
        "    pass\n",
        "  # similarity[key] = (np.mean(post_sim))\n",
        "  # break\n",
        "# embed_query['outputs'][0]\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XescJHWri1DY"
      },
      "source": [
        "similarity = sorted(similarity.items(), key=lambda x: x[1], reverse=True)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cXUSM3LjP8c",
        "outputId": "1f6ac0d6-b541-4f74-c449-93af2fe2ff2f"
      },
      "source": [
        "similarity[:10]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('3724149', (0.41582772, 7)),\n",
              " ('1601416', (0.40137133, 206)),\n",
              " ('3521040', (0.36812702, 1)),\n",
              " ('3480399', (0.3669263, 50)),\n",
              " ('860293', (0.34700236, 94)),\n",
              " ('3813382', (0.34126824, 0)),\n",
              " ('3236014', (0.33970445, 94)),\n",
              " ('3647917', (0.33288568, 10)),\n",
              " ('3668410', (0.33172116, 3)),\n",
              " ('1094221', (0.33100533, 2))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "id": "U3qUjlpPtC6k",
        "outputId": "d1c7ecd2-3ab1-4482-8c91-21fdf5914a10"
      },
      "source": [
        "testing_blogs['1094221'][2]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'i never really experienced car driving during the winter time because people have told that the road are too dangerous and too i think this is perhaps the reason why my mother does not allow me to drive as much as during the i have been driving a lot i do not see any differences with my driving of driving more carfully and slowly when driving on what about the road cars are five times dirtier to compare with than the rest of the i was driving with my dirty i found out how dangerous it was to switch lanes because it was like impossible to glance at my dead spots without giving my full concentration to see right through the dirt on my i think i should really consider getting my car washed tomorrow before heading on the what is it about washing cars during winter when the minute you drive on the the car will be is not it driving with a clean car does not only means greater it also means to have better windows to see the is like a just hope the car washing place will be tomorrow'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MO_nI1Fkfza"
      },
      "source": [
        "sample_sentences = ['I am always interested in reading',\n",
        "                    'It is so cold outside',\n",
        "                    'I purchased books','NLP is interesting','sentence encoder gives embeddings']\n",
        "# query_term = []\n",
        "# sample_query_embedding = embed(query_term)['outputs'][0].numpy().reshape(1,-1)\n",
        "sample_sentence_embedding = [ embed([item])['outputs'][0].numpy().reshape(1,-1) for item in sample_sentences ]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxdj5aXPnAeR",
        "outputId": "e8122f5d-eb94-4cd5-c239-a51c3f9d8a78"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Use PCA to reduce the dimensions of the embedding\n",
        "def pca_transform(sentence_embeddings,dim):\n",
        "    vectors = np.vstack((sentence_embeddings[0], sentence_embeddings[1], sentence_embeddings[2]))\n",
        "    # Set the number of dimensions PCA will reduce to\n",
        "    pca = PCA(n_components=dim)\n",
        "    pca.fit(vectors)\n",
        "    new_pca = pca.transform(vectors)\n",
        "    print(\"original shape:   \", vectors.shape)\n",
        "    print(\"transformed shape:\", new_pca.shape)\n",
        "    return(new_pca)\n",
        "\n",
        "new_dim = pca_transform(sample_sentence_embedding, 2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original shape:    (3, 512)\n",
            "transformed shape: (3, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mfQTh8JBqIQ"
      },
      "source": [
        "# Draw a 2d graph with the new dimensions\n",
        "def two_d_graph(sample_sentences,reduced_dims):\n",
        "    colors = (\"red\", \"green\", \"blue\")\n",
        "    groups = (sample_sentences[0], sample_sentences[1], sample_sentences[2]) \n",
        " \n",
        "    # Create plot\n",
        "    fig = plt.figure()\n",
        "    ax = fig.gca()\n",
        " \n",
        "    for data, color, group in zip(reduced_dims, colors, groups):\n",
        "        x, y = data\n",
        "        ax.scatter(x, y, c=color, edgecolors='none', s=30, label=group)\n",
        " \n",
        "    plt.title('Plot of sentence embeddings')\n",
        "    plt.legend(loc=1)\n",
        "    plt.rcParams[\"figure.figsize\"] = (8,15)\n",
        "    plt.show()\n",
        "\n",
        "two_d_graph(sample_sentences,new_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "FTUoihjZB2BY",
        "outputId": "94c88fa9-600e-4e9c-c9bb-bc6980c83a69"
      },
      "source": [
        "import seaborn as sns\n",
        "def plot_similarity(labels, features, rotation):\n",
        "  corr = np.inner(features, features).reshape(len(labels),len(labels))\n",
        "  sns.set(font_scale=0.9)\n",
        "  g = sns.heatmap(\n",
        "      corr,\n",
        "      xticklabels=labels,\n",
        "      yticklabels=labels,\n",
        "      vmin=0,\n",
        "      vmax=1,\n",
        "      cmap=\"YlOrRd\")\n",
        "  g.set_xticklabels(labels, rotation=rotation)\n",
        "  g.set_title(\"Semantic Textual Similarity\")\n",
        "  # print(corr)\n",
        "\n",
        "def run_and_plot(sentences,embeddings):\n",
        "  # message_embeddings_ = embed(messages_)\n",
        "  plot_similarity(sentences, embeddings, 90)\n",
        "\n",
        "run_and_plot(sample_sentences,sample_sentence_embedding)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-44d0e2fc511b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0mplot_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m90\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mrun_and_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_sentences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msample_sentence_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'sample_sentences' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgE8URT_UW6P"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}